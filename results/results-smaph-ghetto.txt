# First results of the SMAPH-1 implementation (which also uses some SMAPH-S elements)
 * SVM: nonlinear SVC with RBF kernel and C = 1.
 * WAT: used defaults everywhere
 * May 11 2016
 * Numbers:
    * C2W	mac-P/R/F1: 0.379/0.637/0.396 mic-P/R/F1: 0.305/0.584/0.401 TP/FP/FN: 239/544/170 std-P/R/F1: 0.315/0.387/0.300	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.124/0.177/0.087 mic-P/R/F1: 0.032/0.061/0.042 TP/FP/FN: 25/759/384 std-P/R/F1: 0.299/0.364/0.238	SMAPH-S annotator

# First results of the SMAPH-1 implementation (which also uses some SMAPH-S elements)
 * Same config as above.
 * NO pruning performed.
 * May 14 2016
 * Time taken: ~6 minutes (everything cached)
 * Numbers:
    * C2W	mac-P/R/F1: 0.131/0.765/0.194 mic-P/R/F1: 0.099/0.731/0.175 TP/FP/FN: 299/2709/110 std-P/R/F1: 0.160/0.343/0.170	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.039/0.218/0.043 mic-P/R/F1: 0.014/0.103/0.025 TP/FP/FN: 42/2972/367 std-P/R/F1: 0.149/0.382/0.136	SMAPH-S annotator
 * Notes:
    * As expected, recall is quite higher, but not by much. Perhaps we could tune WAT for even
    more recall, since 0.765 is not all that much.
    * C2W precision dropped, but not as much as I (andrei) personally expected.
    * Nevertheless, we should experiment with stricter pruning in the SVM classifier.
    * But first, let's train on BOTH datasets (train A and train B).

# Results using basically the same implementation as above, but trained on both GERDAQ-A and B
 * With pruning.
 * May 14 2016
 * Time taken: ~6 minutes (everything cached as before, but populating higher level cache)
 * Numbers:
   * C2W	mac-P/R/F1: 0.403/0.645/0.415 mic-P/R/F1: 0.320/0.592/0.415 TP/FP/FN: 242/514/167 std-P/R/F1: 0.322/0.388/0.308	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.151/0.190/0.111 mic-P/R/F1: 0.042/0.078/0.055 TP/FP/FN: 32/725/377 std-P/R/F1: 0.322/0.368/0.269	SMAPH-S annotator

# Same as before, but C = 0.01
 * Time taken: ~12 minutes
 * C = 0.01 in the kernelized SVM; precision way lower than before with C = 1;
 * Numbers:
   * C2W	mac-P/R/F1: 0.339/0.653/0.395 mic-P/R/F1: 0.274/0.606/0.378 TP/FP/FN: 248/656/161 std-P/R/F1: 0.291/0.382/0.299	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.090/0.176/0.087 mic-P/R/F1: 0.029/0.064/0.040 TP/FP/FN: 26/880/383 std-P/R/F1: 0.249/0.360/0.239	SMAPH-S annotator

* C = 2.0
 * Time taken: ~14 minutes (other stuff going on)
 * C = 2.0
 * Numbers:
   * C2W	mac-P/R/F1: 0.404/0.639/0.414 mic-P/R/F1: 0.322/0.587/0.416 TP/FP/FN: 240/505/169 std-P/R/F1: 0.326/0.391/0.311	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.140/0.172/0.097 mic-P/R/F1: 0.032/0.059/0.042 TP/FP/FN: 24/722/385 std-P/R/F1: 0.320/0.359/0.262	SMAPH-S annotator

* C = 3.0
 * Numbers:
   * C2W	mac-P/R/F1: 0.402/0.632/0.413 mic-P/R/F1: 0.323/0.584/0.416 TP/FP/FN: 239/502/170 std-P/R/F1: 0.330/0.397/0.317	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.134/0.166/0.094 mic-P/R/F1: 0.030/0.054/0.038 TP/FP/FN: 22/720/387 std-P/R/F1: 0.315/0.355/0.261	SMAPH-S annotator

* C = 0.1
 * Numbers:
   * C2W	mac-P/R/F1: 0.357/0.648/0.398 mic-P/R/F1: 0.299/0.597/0.398 TP/FP/FN: 244/572/165 std-P/R/F1: 0.301/0.384/0.301	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.109/0.174/0.089 mic-P/R/F1: 0.031/0.061/0.041 TP/FP/FN: 25/792/384 std-P/R/F1: 0.279/0.360/0.245	SMAPH-S annotator

* C = 0.25
 * Numbers:
   * C2W	mac-P/R/F1: 0.383/0.645/0.408 mic-P/R/F1: 0.313/0.592/0.410 TP/FP/FN: 242/530/167 std-P/R/F1: 0.312/0.388/0.306	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.127/0.174/0.094 mic-P/R/F1: 0.032/0.061/0.042 TP/FP/FN: 25/748/384 std-P/R/F1: 0.302/0.360/0.254	SMAPH-S annotator

* C = 0.5
 * Numbers:
   * C2W	mac-P/R/F1: 0.397/0.647/0.416 mic-P/R/F1: 0.318/0.594/0.414 TP/FP/FN: 243/521/166 std-P/R/F1: 0.318/0.386/0.306	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.136/0.174/0.098 mic-P/R/F1: 0.033/0.061/0.043 TP/FP/FN: 25/740/384 std-P/R/F1: 0.312/0.360/0.260	SMAPH-S annotator

* Same C (0.5), but enabled QueryMethod.ALL
 * Numbers:
   * C2W	mac-P/R/F1: 0.287/0.667/0.324 mic-P/R/F1: 0.180/0.614/0.279 TP/FP/FN: 251/1140/158 std-P/R/F1: 0.273/0.381/0.261	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.101/0.193/0.073 mic-P/R/F1: 0.024/0.081/0.037 TP/FP/FN: 33/1359/376 std-P/R/F1: 0.262/0.368/0.203	SMAPH-S annotator

* C = 0.75
 * Numbers:
  * C2W	mac-P/R/F1: 0.410/0.645/0.421 mic-P/R/F1: 0.322/0.592/0.417 TP/FP/FN: 242/509/167 std-P/R/F1: 0.323/0.388/0.309	SMAPH-S annotator
  * A2W-SAM	mac-P/R/F1: 0.144/0.174/0.103 mic-P/R/F1: 0.033/0.061/0.043 TP/FP/FN: 25/727/384 std-P/R/F1: 0.321/0.360/0.266	SMAPH-S annotator

* C = 0.7
 * Numbers:
  * C2W	mac-P/R/F1: 0.408/0.645/0.420 mic-P/R/F1: 0.321/0.592/0.416 TP/FP/FN: 242/512/167 std-P/R/F1: 0.323/0.388/0.309	SMAPH-S annotator
  * A2W-SAM	mac-P/R/F1: 0.144/0.174/0.103 mic-P/R/F1: 0.033/0.061/0.043 TP/FP/FN: 25/730/384 std-P/R/F1: 0.321/0.360/0.266	SMAPH-S annotator


========================================================================================================================
Added 10 more features from Taivo's side.
========================================================================================================================


 * SVC; C = 0.7
  * Details:
      SVC(C=0.7, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
  * Train data confusion matrix:
  [[74334  6906]
   [   42   845]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.518/0.642/0.455 mic-P/R/F1: 0.367/0.619/0.460 TP/FP/FN: 253/437/156 std-P/R/F1: 0.359/0.409/0.342	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.333/0.313/0.233 mic-P/R/F1: 0.136/0.230/0.171 TP/FP/FN: 94/596/315 std-P/R/F1: 0.405/0.402/0.337	SMAPH-S annotator

 * SVC; C = 0.5
  * Details:
        SVC(C=0.5, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
  * Train data confusion matrix:
        [[74118  7122]
         [   52   835]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.508/0.645/0.447 mic-P/R/F1: 0.353/0.621/0.450 TP/FP/FN: 254/466/155 std-P/R/F1: 0.357/0.407/0.338	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.317/0.305/0.218 mic-P/R/F1: 0.125/0.220/0.159 TP/FP/FN: 90/630/319 std-P/R/F1: 0.397/0.398/0.325	SMAPH-S annotator

 * SVC; C = 2.0
  * Details:
      SVC(C=2.0, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
  * Train data confusion matrix:
      [[75533  5707]
       [   32   855]]
  * Numbers:
    C2W	mac-P/R/F1: 0.565/0.610/0.446 mic-P/R/F1: 0.379/0.582/0.459 TP/FP/FN: 238/390/171 std-P/R/F1: 0.371/0.419/0.353	SMAPH-S annotator
    A2W-SAM	mac-P/R/F1: 0.385/0.333/0.246 mic-P/R/F1: 0.166/0.254/0.201 TP/FP/FN: 104/524/305 std-P/R/F1: 0.419/0.406/0.341	SMAPH-S annotator

 * SGDClassifier
  * Details:
        SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
               epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
               learning_rate='optimal', loss='log', n_iter=50, n_jobs=1,
               penalty='l1', power_t=0.5, random_state=None, shuffle=True,
               verbose=0, warm_start=False)
  * Train data confusion matrix:
        [[73605  7635]
         [  183   704]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.311/0.727/0.352 mic-P/R/F1: 0.204/0.694/0.316 TP/FP/FN: 284/1105/125 std-P/R/F1: 0.300/0.368/0.279	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.164/0.301/0.147 mic-P/R/F1: 0.068/0.230/0.105 TP/FP/FN: 94/1296/315 std-P/R/F1: 0.303/0.400/0.261	SMAPH-S annotator

* SVC; C = 0.1
  * Details:
        SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
  * Train data confusion matrix:
        [[72727  8513]
         [   80   807]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.431/0.717/0.466 mic-P/R/F1: 0.348/0.677/0.460 TP/FP/FN: 277/518/132 std-P/R/F1: 0.326/0.371/0.318	SMAPH-S annotator
  * Numbers with top-k-snippets = 5:
    * Worse
    * C2W	mac-P/R/F1: 0.416/0.648/0.424 mic-P/R/F1: 0.309/0.604/0.409 TP/FP/FN: 247/552/162 std-P/R/F1: 0.355/0.395/0.328	SMAPH-S annotator
  * Numbers with top-k-snippets = 25 (classic) but with 'watMethod = "base-t"':
    * Better
    * C2W	mac-P/R/F1: 0.429/0.726/0.468 mic-P/R/F1: 0.346/0.692/0.462 TP/FP/FN: 283/534/126 std-P/R/F1: 0.323/0.368/0.317	SMAPH-S annotator
  * Numbers with top-k-snippets = 5 and 'watMethod = "base-t"':
    * <PENDING>

* Same SVC with C = 0.1, but with more features, including f#25
  * LOTS of errors with missing data. Working on trying to fix that.
  * Details:
       SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
  * Train data confusion matrix:
       [[73847  8350]
        [   67   834]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.764/0.167/0.119 mic-P/R/F1: 0.200/0.051/0.082 TP/FP/FN: 21/84/388 std-P/R/F1: 0.413/0.365/0.303	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.754/0.110/0.078 mic-P/R/F1: 0.028/0.005/0.008 TP/FP/FN: 2/70/407 std-P/R/F1: 0.430/0.310/0.265	SMAPH-S annotator
  * Numbers after fixing bug, but NOT regenerating training data:
    * C2W	mac-P/R/F1: 0.407/0.720/0.445 mic-P/R/F1: 0.338/0.689/0.453 TP/FP/FN: 282/553/127 std-P/R/F1: 0.318/0.372/0.312	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.160/0.257/0.142 mic-P/R/F1: 0.085/0.174/0.114 TP/FP/FN: 71/765/338 std-P/R/F1: 0.297/0.386/0.267	SMAPH-S annotator
  * Numbers after also regenerating training data with high-level cache disabled:
    * C2W	mac-P/R/F1: 0.407/0.720/0.445 mic-P/R/F1: 0.338/0.689/0.453 TP/FP/FN: 282/553/127 std-P/R/F1: 0.318/0.372/0.312	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.148/0.246/0.131 mic-P/R/F1: 0.075/0.154/0.101 TP/FP/FN: 63/773/346 std-P/R/F1: 0.290/0.384/0.260	SMAPH-S annotator

  * C := 0.5
  SVC(C=0.5, cache_size=200, class_weight='balanced', coef0=0.0,
    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
  [[75078  7119]
   [   43   858]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.422/0.702/0.431 mic-P/R/F1: 0.342/0.672/0.453 TP/FP/FN: 275/530/134 std-P/R/F1: 0.326/0.385/0.313	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.193/0.278/0.153 mic-P/R/F1: 0.093/0.183/0.123 TP/FP/FN: 75/731/334 std-P/R/F1: 0.321/0.390/0.267	SMAPH-S annotator

  * C := 0.001

  * C := 0.01
      SVC(C=0.01, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
      [[71992 10205]
       [  121   780]]
    * Numbers:
      * C2W	mac-P/R/F1: 0.395/0.722/0.446 mic-P/R/F1: 0.329/0.687/0.445 TP/FP/FN: 281/572/128 std-P/R/F1: 0.313/0.371/0.315	SMAPH-S annotator
      * A2W-SAM	mac-P/R/F1: 0.135/0.246/0.131 mic-P/R/F1: 0.074/0.154/0.100 TP/FP/FN: 63/791/346 std-P/R/F1: 0.270/0.380/0.256	SMAPH-S annotator

  * C := 0.05
        SVC(C=0.05, cache_size=200, class_weight='balanced', coef0=0.0,
            decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
            max_iter=-1, probability=False, random_state=None, shrinking=True,
            tol=0.001, verbose=False)
        [[73331  8866]
         [   87   814]]
     * C2W	mac-P/R/F1: 0.399/0.721/0.446 mic-P/R/F1: 0.336/0.687/0.451 TP/FP/FN: 281/555/128 std-P/R/F1: 0.316/0.372/0.313	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.143/0.250/0.135 mic-P/R/F1: 0.076/0.156/0.103 TP/FP/FN: 64/773/345 std-P/R/F1: 0.284/0.387/0.265	SMAPH-S annotator

 * C := 0.70
   SVC(C=0.7, cache_size=200, class_weight='balanced', coef0=0.0,
     decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
     max_iter=-1, probability=False, random_state=None, shrinking=True,
     tol=0.001, verbose=False)
   [[75431  6766]
    [   39   862]]
    C2W	mac-P/R/F1: 0.425/0.694/0.431 mic-P/R/F1: 0.344/0.665/0.453 TP/FP/FN: 272/519/137 std-P/R/F1: 0.330/0.386/0.316	SMAPH-S annotator
    A2W-SAM	mac-P/R/F1: 0.198/0.282/0.157 mic-P/R/F1: 0.097/0.188/0.128 TP/FP/FN: 77/715/332 std-P/R/F1: 0.325/0.390/0.270	SMAPH-S annotator

 * C := 0.90
  SVC(C=0.9, cache_size=200, class_weight='balanced', coef0=0.0,
    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
  [[75698  6499]
   [   36   865]]
C2W	mac-P/R/F1: 0.432/0.690/0.431 mic-P/R/F1: 0.349/0.660/0.456 TP/FP/FN: 270/504/139 std-P/R/F1: 0.336/0.392/0.320	SMAPH-S annotator
A2W-SAM	mac-P/R/F1: 0.216/0.301/0.171 mic-P/R/F1: 0.108/0.205/0.142 TP/FP/FN: 84/691/325 std-P/R/F1: 0.336/0.399/0.280	SMAPH-S annotator

 * C := 1.50
  SVC(C=1.5, cache_size=200, class_weight='balanced', coef0=0.0,
    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
  [[76292  5905]
   [   25   876]]
   * Numbers
   C2W	mac-P/R/F1: 0.438/0.663/0.423 mic-P/R/F1: 0.352/0.641/0.454 TP/FP/FN: 262/482/147 std-P/R/F1: 0.342/0.401/0.322	SMAPH-S annotator
   A2W-SAM	mac-P/R/F1: 0.237/0.314/0.187 mic-P/R/F1: 0.126/0.230/0.163 TP/FP/FN: 94/650/315 std-P/R/F1: 0.344/0.406/0.290	SMAPH-S annotator


