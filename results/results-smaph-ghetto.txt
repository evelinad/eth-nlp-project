# SMAPH results using the SVM step.
Note: this file is written into as an experiment log, mostly by Andrei. The lower you go, the
more up to date the information is (i.e. better algorithm, more features, less bugs, more data,
etc.).

# First results of the SMAPH-1 implementation (which also uses some SMAPH-S elements)
 * SVM: nonlinear SVC with RBF kernel and C = 1.
 * WAT: used defaults everywhere
 * May 11 2016
 * Numbers:
    * C2W	mac-P/R/F1: 0.379/0.637/0.396 mic-P/R/F1: 0.305/0.584/0.401 TP/FP/FN: 239/544/170 std-P/R/F1: 0.315/0.387/0.300	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.124/0.177/0.087 mic-P/R/F1: 0.032/0.061/0.042 TP/FP/FN: 25/759/384 std-P/R/F1: 0.299/0.364/0.238	SMAPH-S annotator

# First results of the SMAPH-1 implementation (which also uses some SMAPH-S elements)
 * Same config as above.
 * NO pruning performed.
 * May 14 2016
 * Time taken: ~6 minutes (everything cached)
 * Numbers:
    * C2W	mac-P/R/F1: 0.131/0.765/0.194 mic-P/R/F1: 0.099/0.731/0.175 TP/FP/FN: 299/2709/110 std-P/R/F1: 0.160/0.343/0.170	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.039/0.218/0.043 mic-P/R/F1: 0.014/0.103/0.025 TP/FP/FN: 42/2972/367 std-P/R/F1: 0.149/0.382/0.136	SMAPH-S annotator
 * Notes:
    * As expected, recall is quite higher, but not by much. Perhaps we could tune WAT for even
    more recall, since 0.765 is not all that much.
    * C2W precision dropped, but not as much as I (andrei) personally expected.
    * Nevertheless, we should experiment with stricter pruning in the SVM classifier.
    * But first, let's train on BOTH datasets (train A and train B).

# Results using basically the same implementation as above, but trained on both GERDAQ-A and B
 * With pruning.
 * May 14 2016
 * Time taken: ~6 minutes (everything cached as before, but populating higher level cache)
 * Numbers:
   * C2W	mac-P/R/F1: 0.403/0.645/0.415 mic-P/R/F1: 0.320/0.592/0.415 TP/FP/FN: 242/514/167 std-P/R/F1: 0.322/0.388/0.308	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.151/0.190/0.111 mic-P/R/F1: 0.042/0.078/0.055 TP/FP/FN: 32/725/377 std-P/R/F1: 0.322/0.368/0.269	SMAPH-S annotator

# Same as before, but C = 0.01
 * Time taken: ~12 minutes
 * C = 0.01 in the kernelized SVM; precision way lower than before with C = 1;
 * Numbers:
   * C2W	mac-P/R/F1: 0.339/0.653/0.395 mic-P/R/F1: 0.274/0.606/0.378 TP/FP/FN: 248/656/161 std-P/R/F1: 0.291/0.382/0.299	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.090/0.176/0.087 mic-P/R/F1: 0.029/0.064/0.040 TP/FP/FN: 26/880/383 std-P/R/F1: 0.249/0.360/0.239	SMAPH-S annotator

* C = 2.0
 * Time taken: ~14 minutes (other stuff going on)
 * C = 2.0
 * Numbers:
   * C2W	mac-P/R/F1: 0.404/0.639/0.414 mic-P/R/F1: 0.322/0.587/0.416 TP/FP/FN: 240/505/169 std-P/R/F1: 0.326/0.391/0.311	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.140/0.172/0.097 mic-P/R/F1: 0.032/0.059/0.042 TP/FP/FN: 24/722/385 std-P/R/F1: 0.320/0.359/0.262	SMAPH-S annotator

* C = 3.0
 * Numbers:
   * C2W	mac-P/R/F1: 0.402/0.632/0.413 mic-P/R/F1: 0.323/0.584/0.416 TP/FP/FN: 239/502/170 std-P/R/F1: 0.330/0.397/0.317	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.134/0.166/0.094 mic-P/R/F1: 0.030/0.054/0.038 TP/FP/FN: 22/720/387 std-P/R/F1: 0.315/0.355/0.261	SMAPH-S annotator

* C = 0.1
 * Numbers:
   * C2W	mac-P/R/F1: 0.357/0.648/0.398 mic-P/R/F1: 0.299/0.597/0.398 TP/FP/FN: 244/572/165 std-P/R/F1: 0.301/0.384/0.301	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.109/0.174/0.089 mic-P/R/F1: 0.031/0.061/0.041 TP/FP/FN: 25/792/384 std-P/R/F1: 0.279/0.360/0.245	SMAPH-S annotator

* C = 0.25
 * Numbers:
   * C2W	mac-P/R/F1: 0.383/0.645/0.408 mic-P/R/F1: 0.313/0.592/0.410 TP/FP/FN: 242/530/167 std-P/R/F1: 0.312/0.388/0.306	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.127/0.174/0.094 mic-P/R/F1: 0.032/0.061/0.042 TP/FP/FN: 25/748/384 std-P/R/F1: 0.302/0.360/0.254	SMAPH-S annotator

* C = 0.5
 * Numbers:
   * C2W	mac-P/R/F1: 0.397/0.647/0.416 mic-P/R/F1: 0.318/0.594/0.414 TP/FP/FN: 243/521/166 std-P/R/F1: 0.318/0.386/0.306	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.136/0.174/0.098 mic-P/R/F1: 0.033/0.061/0.043 TP/FP/FN: 25/740/384 std-P/R/F1: 0.312/0.360/0.260	SMAPH-S annotator

* Same C (0.5), but enabled QueryMethod.ALL
 * Numbers:
   * C2W	mac-P/R/F1: 0.287/0.667/0.324 mic-P/R/F1: 0.180/0.614/0.279 TP/FP/FN: 251/1140/158 std-P/R/F1: 0.273/0.381/0.261	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.101/0.193/0.073 mic-P/R/F1: 0.024/0.081/0.037 TP/FP/FN: 33/1359/376 std-P/R/F1: 0.262/0.368/0.203	SMAPH-S annotator

* C = 0.75
 * Numbers:
  * C2W	mac-P/R/F1: 0.410/0.645/0.421 mic-P/R/F1: 0.322/0.592/0.417 TP/FP/FN: 242/509/167 std-P/R/F1: 0.323/0.388/0.309	SMAPH-S annotator
  * A2W-SAM	mac-P/R/F1: 0.144/0.174/0.103 mic-P/R/F1: 0.033/0.061/0.043 TP/FP/FN: 25/727/384 std-P/R/F1: 0.321/0.360/0.266	SMAPH-S annotator

* C = 0.7
 * Numbers:
  * C2W	mac-P/R/F1: 0.408/0.645/0.420 mic-P/R/F1: 0.321/0.592/0.416 TP/FP/FN: 242/512/167 std-P/R/F1: 0.323/0.388/0.309	SMAPH-S annotator
  * A2W-SAM	mac-P/R/F1: 0.144/0.174/0.103 mic-P/R/F1: 0.033/0.061/0.043 TP/FP/FN: 25/730/384 std-P/R/F1: 0.321/0.360/0.266	SMAPH-S annotator


========================================================================================================================
Added 10 more features from Taivo's side.
========================================================================================================================


 * SVC; C = 0.7
  * Details:
      SVC(C=0.7, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
  * Train data confusion matrix:
  [[74334  6906]
   [   42   845]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.518/0.642/0.455 mic-P/R/F1: 0.367/0.619/0.460 TP/FP/FN: 253/437/156 std-P/R/F1: 0.359/0.409/0.342	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.333/0.313/0.233 mic-P/R/F1: 0.136/0.230/0.171 TP/FP/FN: 94/596/315 std-P/R/F1: 0.405/0.402/0.337	SMAPH-S annotator

 * SVC; C = 0.5
  * Details:
        SVC(C=0.5, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
  * Train data confusion matrix:
        [[74118  7122]
         [   52   835]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.508/0.645/0.447 mic-P/R/F1: 0.353/0.621/0.450 TP/FP/FN: 254/466/155 std-P/R/F1: 0.357/0.407/0.338	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.317/0.305/0.218 mic-P/R/F1: 0.125/0.220/0.159 TP/FP/FN: 90/630/319 std-P/R/F1: 0.397/0.398/0.325	SMAPH-S annotator

 * SVC; C = 2.0
  * Details:
      SVC(C=2.0, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
  * Train data confusion matrix:
      [[75533  5707]
       [   32   855]]
  * Numbers:
    C2W	mac-P/R/F1: 0.565/0.610/0.446 mic-P/R/F1: 0.379/0.582/0.459 TP/FP/FN: 238/390/171 std-P/R/F1: 0.371/0.419/0.353	SMAPH-S annotator
    A2W-SAM	mac-P/R/F1: 0.385/0.333/0.246 mic-P/R/F1: 0.166/0.254/0.201 TP/FP/FN: 104/524/305 std-P/R/F1: 0.419/0.406/0.341	SMAPH-S annotator

 * SGDClassifier
  * Details:
        SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
               epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
               learning_rate='optimal', loss='log', n_iter=50, n_jobs=1,
               penalty='l1', power_t=0.5, random_state=None, shuffle=True,
               verbose=0, warm_start=False)
  * Train data confusion matrix:
        [[73605  7635]
         [  183   704]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.311/0.727/0.352 mic-P/R/F1: 0.204/0.694/0.316 TP/FP/FN: 284/1105/125 std-P/R/F1: 0.300/0.368/0.279	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.164/0.301/0.147 mic-P/R/F1: 0.068/0.230/0.105 TP/FP/FN: 94/1296/315 std-P/R/F1: 0.303/0.400/0.261	SMAPH-S annotator

* SVC; C = 0.1
  * Details:
        SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
  * Train data confusion matrix:
        [[72727  8513]
         [   80   807]]
  * Numbers:
    * C2W	mac-P/R/F1: 0.431/0.717/0.466 mic-P/R/F1: 0.348/0.677/0.460 TP/FP/FN: 277/518/132 std-P/R/F1: 0.326/0.371/0.318	SMAPH-S annotator
  * Numbers with top-k-snippets = 5:
    * Worse
    * C2W	mac-P/R/F1: 0.416/0.648/0.424 mic-P/R/F1: 0.309/0.604/0.409 TP/FP/FN: 247/552/162 std-P/R/F1: 0.355/0.395/0.328	SMAPH-S annotator
  * Numbers with top-k-snippets = 25 (classic) but with 'watMethod = "base-t"':
    * Better
    * C2W	mac-P/R/F1: 0.429/0.726/0.468 mic-P/R/F1: 0.346/0.692/0.462 TP/FP/FN: 283/534/126 std-P/R/F1: 0.323/0.368/0.317	SMAPH-S annotator
  * Numbers with top-k-snippets = 5 and 'watMethod = "base-t"':
    * <not evaluated>

========================================================================================================================
Added new features to set
========================================================================================================================

 * C := 0.001
   * Details:
       SVC(C=0.001, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[70184 12013]
        [  156   745]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.355/0.713/0.396 mic-P/R/F1: 0.242/0.682/0.357 TP/FP/FN: 279/876/130 std-P/R/F1: 0.321/0.372/0.312	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.142/0.267/0.135 mic-P/R/F1: 0.063/0.178/0.093 TP/FP/FN: 73/1084/336 std-P/R/F1: 0.275/0.387/0.254	SMAPH-S annotator

 * C := 0.01
   * Details
      SVC(C=0.01, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
   * Train data confusion matrix:
      [[71992 10205]
       [  121   780]]
   * Numbers:
      * C2W	mac-P/R/F1: 0.395/0.722/0.446 mic-P/R/F1: 0.329/0.687/0.445 TP/FP/FN: 281/572/128 std-P/R/F1: 0.313/0.371/0.315	SMAPH-S annotator
      * A2W-SAM	mac-P/R/F1: 0.135/0.246/0.131 mic-P/R/F1: 0.074/0.154/0.100 TP/FP/FN: 63/791/346 std-P/R/F1: 0.270/0.380/0.256	SMAPH-S annotator

 * C := 0.05
   * Details:
        SVC(C=0.05, cache_size=200, class_weight='balanced', coef0=0.0,
            decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
            max_iter=-1, probability=False, random_state=None, shrinking=True,
            tol=0.001, verbose=False)
   * Train data confusion matrix:
        [[73331  8866]
         [   87   814]]
   * Numbers
     * C2W	mac-P/R/F1: 0.399/0.721/0.446 mic-P/R/F1: 0.336/0.687/0.451 TP/FP/FN: 281/555/128 std-P/R/F1: 0.316/0.372/0.313	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.143/0.250/0.135 mic-P/R/F1: 0.076/0.156/0.103 TP/FP/FN: 64/773/345 std-P/R/F1: 0.284/0.387/0.265	SMAPH-S annotator

 * Same SVC with C = 0.1, but with more features, including f#25
   * LOTS of errors with missing data. Working on trying to fix that.
   * Details:
       SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[73847  8350]
        [   67   834]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.764/0.167/0.119 mic-P/R/F1: 0.200/0.051/0.082 TP/FP/FN: 21/84/388 std-P/R/F1: 0.413/0.365/0.303	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.754/0.110/0.078 mic-P/R/F1: 0.028/0.005/0.008 TP/FP/FN: 2/70/407 std-P/R/F1: 0.430/0.310/0.265	SMAPH-S annotator
   * Numbers after fixing bug, but NOT regenerating training data:
     * C2W	mac-P/R/F1: 0.407/0.720/0.445 mic-P/R/F1: 0.338/0.689/0.453 TP/FP/FN: 282/553/127 std-P/R/F1: 0.318/0.372/0.312	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.160/0.257/0.142 mic-P/R/F1: 0.085/0.174/0.114 TP/FP/FN: 71/765/338 std-P/R/F1: 0.297/0.386/0.267	SMAPH-S annotator
   * Numbers after also regenerating training data with high-level cache disabled:
     * C2W	mac-P/R/F1: 0.407/0.720/0.445 mic-P/R/F1: 0.338/0.689/0.453 TP/FP/FN: 282/553/127 std-P/R/F1: 0.318/0.372/0.312	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.148/0.246/0.131 mic-P/R/F1: 0.075/0.154/0.101 TP/FP/FN: 63/773/346 std-P/R/F1: 0.290/0.384/0.260	SMAPH-S annotator

 * C := 0.5
   * Details:
      SVC(C=0.5, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
   * Train data confusion matrix:
      [[75078  7119]
       [   43   858]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.422/0.702/0.431 mic-P/R/F1: 0.342/0.672/0.453 TP/FP/FN: 275/530/134 std-P/R/F1: 0.326/0.385/0.313	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.193/0.278/0.153 mic-P/R/F1: 0.093/0.183/0.123 TP/FP/FN: 75/731/334 std-P/R/F1: 0.321/0.390/0.267	SMAPH-S annotator

 * C := 0.70
   * Details
       SVC(C=0.7, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[75431  6766]
        [   39   862]]
   * Numbers
     * C2W	mac-P/R/F1: 0.425/0.694/0.431 mic-P/R/F1: 0.344/0.665/0.453 TP/FP/FN: 272/519/137 std-P/R/F1: 0.330/0.386/0.316	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.198/0.282/0.157 mic-P/R/F1: 0.097/0.188/0.128 TP/FP/FN: 77/715/332 std-P/R/F1: 0.325/0.390/0.270	SMAPH-S annotator

 * C := 0.90
   * Details:
      SVC(C=0.9, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
   * Train data confusion matrix:
      [[75698  6499]
       [   36   865]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.432/0.690/0.431 mic-P/R/F1: 0.349/0.660/0.456 TP/FP/FN: 270/504/139 std-P/R/F1: 0.336/0.392/0.320	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.216/0.301/0.171 mic-P/R/F1: 0.108/0.205/0.142 TP/FP/FN: 84/691/325 std-P/R/F1: 0.336/0.399/0.280	SMAPH-S annotator
   * With k = 100 (took ~3x more time):
     * C2W	mac-P/R/F1: 0.988/0.192/0.202 mic-P/R/F1: 0.914/0.078/0.144 TP/FP/FN: 32/3/377 std-P/R/F1: 0.110/0.369/0.378	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.890/0.127/0.126 mic-P/R/F1: 0.229/0.020/0.036 TP/FP/FN: 8/27/401 std-P/R/F1: 0.313/0.326/0.325	SMAPH-S annotator
   * With k = 40:
     * C2W	mac-P/R/F1: 0.640/0.427/0.384 mic-P/R/F1: 0.454/0.352/0.397 TP/FP/FN: 144/173/265 std-P/R/F1: 0.409/0.423/0.391	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.369/0.162/0.132 mic-P/R/F1: 0.076/0.059/0.066 TP/FP/FN: 24/293/385 std-P/R/F1: 0.468/0.344/0.311	SMAPH-S annotator
   * With k = 35:
     * C2W	mac-P/R/F1: 0.440/0.593/0.405 mic-P/R/F1: 0.319/0.540/0.401 TP/FP/FN: 221/471/188 std-P/R/F1: 0.353/0.402/0.322	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.226/0.250/0.161 mic-P/R/F1: 0.092/0.156/0.116 TP/FP/FN: 64/628/345 std-P/R/F1: 0.363/0.381/0.290	SMAPH-S annotator
   * With k = 30:
     * C2W	mac-P/R/F1: 0.411/0.666/0.410 mic-P/R/F1: 0.320/0.636/0.426 TP/FP/FN: 260/552/149 std-P/R/F1: 0.330/0.392/0.311	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.202/0.268/0.153 mic-P/R/F1: 0.097/0.193/0.129 TP/FP/FN: 79/734/330 std-P/R/F1: 0.333/0.386/0.271	SMAPH-S annotator
   * With k = 20:
     * C2W	mac-P/R/F1: 0.439/0.657/0.415 mic-P/R/F1: 0.326/0.623/0.428 TP/FP/FN: 255/527/154 std-P/R/F1: 0.344/0.400/0.319	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.305/0.408/0.254 mic-P/R/F1: 0.178/0.340/0.233 TP/FP/FN: 139/643/270 std-P/R/F1: 0.361/0.419/0.312	SMAPH-S annotator

 * C := 1.50
   * Details:
      SVC(C=1.5, cache_size=200, class_weight='balanced', coef0=0.0,
        decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
        max_iter=-1, probability=False, random_state=None, shrinking=True,
        tol=0.001, verbose=False)
   * Train data confusion matrix:
      [[76292  5905]
       [   25   876]]
   * Numbers
    * C2W	mac-P/R/F1: 0.438/0.663/0.423 mic-P/R/F1: 0.352/0.641/0.454 TP/FP/FN: 262/482/147 std-P/R/F1: 0.342/0.401/0.322	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.237/0.314/0.187 mic-P/R/F1: 0.126/0.230/0.163 TP/FP/FN: 94/650/315 std-P/R/F1: 0.344/0.406/0.290	SMAPH-S annotator

========================================================================================================================
Fixed duplicate positive label gen bug
========================================================================================================================

 * SGDClassifier
   * Note: very likely to include some of the yahoo data already!
   * Details:
       Classifier: SGDClassifier(alpha=0.025, average=False, class_weight='balanced',
              epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
              learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
              penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,
              verbose=0, warm_start=False)
   * Numbers:
     * C2W	mac-P/R/F1: 0.427/0.695/0.443 mic-P/R/F1: 0.348/0.660/0.456 TP/FP/FN: 270/505/139 std-P/R/F1: 0.318/0.376/0.307	SMAPH-S annotator
     * BUG FIX => WAY, WAY better A2W SCORE!
     * A2W-SAM	mac-P/R/F1: 0.294/0.442/0.281 mic-P/R/F1: 0.214/0.406/0.280 TP/FP/FN: 166/609/243 std-P/R/F1: 0.334/0.418/0.313	SMAPH-S annotator

 * SVC; C = 0.7
   * Details:
        SVC(C=0.7, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
   * Train data confusion matrix:
        [[118703   3832]
         [    37   1499]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.443/0.682/0.443 mic-P/R/F1: 0.368/0.650/0.470 TP/FP/FN: 266/457/143 std-P/R/F1: 0.343/0.393/0.327	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.332/0.486/0.312 mic-P/R/F1: 0.257/0.455/0.328 TP/FP/FN: 186/538/223 std-P/R/F1: 0.349/0.423/0.325	SMAPH-S annotator

 * SVC; C = 1.2
   * Details:
     * SVC(C=1.2, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
     * Train data confusion matrix:
       [[119141   3394]
        [    27   1509]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.448/0.668/0.443 mic-P/R/F1: 0.376/0.636/0.472 TP/FP/FN: 260/432/149 std-P/R/F1: 0.351/0.398/0.333	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.344/0.492/0.321 mic-P/R/F1: 0.271/0.460/0.341 TP/FP/FN: 188/505/221 std-P/R/F1: 0.356/0.421/0.329	SMAPH-S annotator

 * SVC; C = 0.1 (should be good according to CV)
   * Details:
       SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[117663   4872]
        [    71   1465]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.430/0.702/0.455 mic-P/R/F1: 0.358/0.670/0.467 TP/FP/FN: 274/491/135 std-P/R/F1: 0.328/0.378/0.318	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.309/0.475/0.308 mic-P/R/F1: 0.239/0.447/0.312 TP/FP/FN: 183/582/226 std-P/R/F1: 0.342/0.422/0.329	SMAPH-S annotator

 * SVC; C = 0.01
   * Details:
        SVC(C=0.01, cache_size=200, class_weight='balanced', coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
          max_iter=-1, probability=False, random_state=None, shrinking=True,
          tol=0.001, verbose=False)
   * Train data confusion matrix:
        [[116560   5975]
         [   123   1413]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.434/0.697/0.460 mic-P/R/F1: 0.366/0.658/0.470 TP/FP/FN: 269/466/140 std-P/R/F1: 0.331/0.376/0.321	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.308/0.468/0.309 mic-P/R/F1: 0.242/0.435/0.311 TP/FP/FN: 178/558/231 std-P/R/F1: 0.340/0.416/0.329	SMAPH-S annotator*

 * SVC; C = 0.001
   * Details:
       SVC(C=0.001, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[115478   7057]
        [   197   1339]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.480/0.668/0.469 mic-P/R/F1: 0.399/0.626/0.487 TP/FP/FN: 256/386/153 std-P/R/F1: 0.352/0.382/0.333	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.344/0.440/0.311 mic-P/R/F1: 0.257/0.403/0.314 TP/FP/FN: 165/477/244 std-P/R/F1: 0.376/0.420/0.349	SMAPH-S annotator

 * SVC; C = 0.0005
     SVC(C=0.0005, cache_size=200, class_weight='balanced', coef0=0.0,
       decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
       max_iter=-1, probability=False, random_state=None, shrinking=True,
       tol=0.001, verbose=False)
     [[115614   6921]
      [   219   1317]]
   C2W	mac-P/R/F1: 0.477/0.657/0.460 mic-P/R/F1: 0.390/0.616/0.478 TP/FP/FN: 252/394/157 std-P/R/F1: 0.353/0.386/0.331	SMAPH-S annotator
   A2W-SAM	mac-P/R/F1: 0.338/0.426/0.299 mic-P/R/F1: 0.245/0.386/0.300 TP/FP/FN: 158/488/251 std-P/R/F1: 0.377/0.419/0.344	SMAPH-S annotator

 * SVC; C = 0.0001
   * Details:
       SVC(C=0.0001, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[116369   6166]
        [   285   1251]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.506/0.623/0.448 mic-P/R/F1: 0.372/0.587/0.455 TP/FP/FN: 240/405/169 std-P/R/F1: 0.377/0.401/0.349	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.382/0.423/0.307 mic-P/R/F1: 0.242/0.381/0.296 TP/FP/FN: 156/489/253 std-P/R/F1: 0.401/0.415/0.351	SMAPH-S annotatorng>

========================================================================================================================
Now also added MORE Yahoo! queries up to a total of 2.2k train queries
========================================================================================================================

 * SGDC:
   * Details:
       SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
              eta0=0.0, fit_intercept=True, l1_ratio=0.15,
              learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
              penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,
              verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[180881   9036]
        [   306   2490]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.451/0.676/0.457 mic-P/R/F1: 0.365/0.633/0.463 TP/FP/FN: 259/451/150 std-P/R/F1: 0.336/0.381/0.318	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.329/0.467/0.316 mic-P/R/F1: 0.249/0.433/0.316 TP/FP/FN: 177/533/232 std-P/R/F1: 0.351/0.421/0.332	SMAPH-S annotator
   * Numbers with coverage maximization (might be a little buggy):
     * C2W	mac-P/R/F1: 0.474/0.529/0.429 mic-P/R/F1: 0.412/0.494/0.449 TP/FP/FN: 202/288/207 std-P/R/F1: 0.397/0.411/0.373	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.293/0.350/0.256 mic-P/R/F1: 0.215/0.315/0.255 TP/FP/FN: 129/472/280 std-P/R/F1: 0.365/0.394/0.329	SMAPH-S annotator
   * After bugfix:
     * C2W	mac-P/R/F1: 0.555/0.517/0.461 mic-P/R/F1: 0.512/0.474/0.492 TP/FP/FN: 194/185/215 std-P/R/F1: 0.438/0.410/0.396	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.371/0.395/0.320 mic-P/R/F1: 0.341/0.345/0.343 TP/FP/FN: 141/273/268 std-P/R/F1: 0.417/0.426/0.385	SMAPH-S annotator


 * SGDC:
   * Details:
       SGDClassifier(alpha=0.0005, average=False, class_weight='balanced',
              epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
              learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
              penalty='l1', power_t=0.5, random_state=None, shuffle=True,
              verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[178503  11414]
        [   242   2554]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.333/0.702/0.378 mic-P/R/F1: 0.256/0.672/0.371 TP/FP/FN: 275/798/134 std-P/R/F1: 0.291/0.374/0.284	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.255/0.504/0.274 mic-P/R/F1: 0.182/0.477/0.263 TP/FP/FN: 195/879/214 std-P/R/F1: 0.294/0.416/0.285	SMAPH-S annotator

 * SVC; C = 0.001:
   * Details:
       SVC(C=0.001, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
       [[177972  11945]
        [   331   2465]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.480/0.668/0.467 mic-P/R/F1: 0.397/0.626/0.486 TP/FP/FN: 256/389/153 std-P/R/F1: 0.350/0.382/0.332	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.346/0.439/0.310 mic-P/R/F1: 0.256/0.403/0.313 TP/FP/FN: 165/480/244 std-P/R/F1: 0.376/0.418/0.346	SMAPH-S annotator

 * SVC; C = 0.0005
   * Details:
   * Numbers:
     * C2W	mac-P/R/F1: 0.588/0.494/0.453 mic-P/R/F1: 0.561/0.428/0.485 TP/FP/FN: 175/137/234 std-P/R/F1: 0.458/0.410/0.402	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.333/0.332/0.263 mic-P/R/F1: 0.324/0.262/0.290 TP/FP/FN: 107/223/302 std-P/R/F1: 0.436/0.428/0.387	SMAPH-S annotator

========================================================================================================================
After doing the refactor to support Taivo's changes
========================================================================================================================
 * SVC; C = 0.0005; Using naive annotation picker.
   * Still using SMV trained on part of Yahoo! WebScope data.
   * Details:
   * Numbers:
     C2W	mac-P/R/F1: 0.489/0.664/0.470 mic-P/R/F1: 0.407/0.623/0.492 TP/FP/FN: 255/372/154 std-P/R/F1: 0.355/0.384/0.334	SMAPH-S annotator
     A2W-SAM	mac-P/R/F1: 0.345/0.439/0.307 mic-P/R/F1: 0.258/0.396/0.313 TP/FP/FN: 162/465/247 std-P/R/F1: 0.376/0.424/0.344	SMAPH-S annotator

 * SGDClassifier
   * Still using SMV trained on part of Yahoo! WebScope data.
   * Details:
       SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
              eta0=0.0, fit_intercept=True, l1_ratio=0.15,
              learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
              penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,
              verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[180881   9036]
        [   306   2490]]
   * Numbers (same as before):
     * C2W	mac-P/R/F1: 0.451/0.676/0.457 mic-P/R/F1: 0.365/0.633/0.463 TP/FP/FN: 259/451/150 std-P/R/F1: 0.336/0.381/0.318	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.327/0.463/0.312 mic-P/R/F1: 0.245/0.425/0.311 TP/FP/FN: 174/536/235 std-P/R/F1: 0.355/0.422/0.332	SMAPH-S annotator
   * Numbers (using Smaph-S pruning with threshold 0.2):
     * C2W	mac-P/R/F1: 0.382/0.518/0.375 mic-P/R/F1: 0.324/0.457/0.379 TP/FP/FN: 187/390/222 std-P/R/F1: 0.367/0.408/0.351	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.223/0.306/0.206 mic-P/R/F1: 0.170/0.240/0.199 TP/FP/FN: 98/480/311 std-P/R/F1: 0.325/0.388/0.304	SMAPH-S annotator

 * SVC; C = 0.0005; Probabilistic
   * Details:
       SVC(C=0.0005, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=True, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
        [[77307  5169]
         [   99   523]]
   * Numbers (NO smaph-S pruning; naive mention picker):
     * C2W	mac-P/R/F1: 0.404/0.684/0.423 mic-P/R/F1: 0.271/0.643/0.382 TP/FP/FN: 263/706/146 std-P/R/F1: 0.340/0.377/0.322	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.266/0.399/0.255 mic-P/R/F1: 0.145/0.345/0.204 TP/FP/FN: 141/829/268 std-P/R/F1: 0.346/0.418/0.323	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.2; naive mention picker):
     * smaph-S pruning == use SmaphSRemoteSvmPruner
     * C2W	mac-P/R/F1: 0.724/0.241/0.259 mic-P/R/F1: 0.433/0.191/0.265 TP/FP/FN: 78/102/331 std-P/R/F1: 0.422/0.367/0.378	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.693/0.212/0.231 mic-P/R/F1: 0.356/0.156/0.217 TP/FP/FN: 64/116/345 std-P/R/F1: 0.439/0.348/0.365	SMAPH-S annotator
     * Naive mention picker existence turns out not to matter.

   * Numbers (with smaph-S pruning threshold 0.50; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.912/0.144/0.155 mic-P/R/F1: 0.472/0.061/0.108 TP/FP/FN: 25/28/384 std-P/R/F1: 0.275/0.321/0.333	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.904/0.140/0.150 mic-P/R/F1: 0.415/0.054/0.095 TP/FP/FN: 22/31/387 std-P/R/F1: 0.287/0.319/0.330	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.25; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.744/0.230/0.250 mic-P/R/F1: 0.436/0.176/0.251 TP/FP/FN: 72/93/337 std-P/R/F1: 0.413/0.359/0.371	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.716/0.205/0.225 mic-P/R/F1: 0.361/0.147/0.209 TP/FP/FN: 60/106/349 std-P/R/F1: 0.431/0.343/0.360	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.20; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.724/0.241/0.259 mic-P/R/F1: 0.433/0.191/0.265 TP/FP/FN: 78/102/331 std-P/R/F1: 0.422/0.367/0.378	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.692/0.212/0.231 mic-P/R/F1: 0.354/0.156/0.217 TP/FP/FN: 64/117/345 std-P/R/F1: 0.440/0.348/0.365	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.15; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.725/0.278/0.298 mic-P/R/F1: 0.467/0.225/0.304 TP/FP/FN: 92/105/317 std-P/R/F1: 0.418/0.380/0.389	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.673/0.232/0.252 mic-P/R/F1: 0.360/0.178/0.239 TP/FP/FN: 73/130/336 std-P/R/F1: 0.445/0.356/0.371	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.10; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.714/0.329/0.343 mic-P/R/F1: 0.484/0.267/0.344 TP/FP/FN: 109/116/300 std-P/R/F1: 0.417/0.396/0.398	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.623/0.251/0.264 mic-P/R/F1: 0.321/0.193/0.241 TP/FP/FN: 79/167/330 std-P/R/F1: 0.459/0.367/0.377	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.05; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.644/0.379/0.362 mic-P/R/F1: 0.466/0.315/0.376 TP/FP/FN: 129/148/280 std-P/R/F1: 0.433/0.403/0.388	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.503/0.261/0.241 mic-P/R/F1: 0.246/0.203/0.223 TP/FP/FN: 83/254/326 std-P/R/F1: 0.466/0.370/0.355	SMAPH-S annotator
     * After bugfix:
       * C2W	mac-P/R/F1: 0.570/0.534/0.456 mic-P/R/F1: 0.510/0.506/0.508 TP/FP/FN: 207/199/202 std-P/R/F1: 0.417/0.411/0.392	SMAPH-S annotator
       * A2W-SAM	mac-P/R/F1: 0.472/0.450/0.368 mic-P/R/F1: 0.374/0.421/0.396 TP/FP/FN: 172/288/237 std-P/R/F1: 0.425/0.412/0.381	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.03; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.615/0.404/0.374 mic-P/R/F1: 0.451/0.337/0.386 TP/FP/FN: 138/168/271 std-P/R/F1: 0.433/0.404/0.382	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.451/0.265/0.233 mic-P/R/F1: 0.220/0.210/0.215 TP/FP/FN: 86/305/323 std-P/R/F1: 0.458/0.369/0.343	SMAPH-S annotator
     * After bugfix:
       * C2W	mac-P/R/F1: 0.558/0.561/0.472 mic-P/R/F1: 0.506/0.531/0.518 TP/FP/FN: 217/212/192 std-P/R/F1: 0.412/0.403/0.385	SMAPH-S annotator
       * A2W-SAM	mac-P/R/F1: 0.443/0.460/0.368 mic-P/R/F1: 0.348/0.433/0.386 TP/FP/FN: 177/331/232 std-P/R/F1: 0.417/0.410/0.377	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.01; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.524/0.461/0.405 mic-P/R/F1: 0.424/0.389/0.406 TP/FP/FN: 159/216/250 std-P/R/F1: 0.429/0.405/0.378	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.287/0.279/0.209 mic-P/R/F1: 0.159/0.220/0.184 TP/FP/FN: 90/477/319 std-P/R/F1: 0.388/0.376/0.320	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.005; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.491/0.482/0.409 mic-P/R/F1: 0.409/0.408/0.409 TP/FP/FN: 167/241/242 std-P/R/F1: 0.423/0.401/0.373	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.228/0.283/0.193 mic-P/R/F1: 0.147/0.225/0.178 TP/FP/FN: 92/535/317 std-P/R/F1: 0.342/0.376/0.302	SMAPH-S annotator
   * Numbers (with smaph-S pruning threshold 0.000; NO extra mention picker):
     * C2W	mac-P/R/F1: 0.439/0.495/0.389 mic-P/R/F1: 0.387/0.421/0.403 TP/FP/FN: 172/273/237 std-P/R/F1: 0.412/0.399/0.360	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.161/0.283/0.161 mic-P/R/F1: 0.133/0.225/0.167 TP/FP/FN: 92/600/317 std-P/R/F1: 0.271/0.376/0.263	SMAPH-S annotator

 * SVC; C = 0.7; Non-probabilistic
   * Using naive annotation picker and NO Smaph-S-style pruning:
   * Details:
       SVC(C=0.7, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
     * This looks very overfit to me (andrei).
       [[80432  2044]
        [    5   617]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.431/0.653/0.421 mic-P/R/F1: 0.350/0.628/0.449 TP/FP/FN: 257/478/152 std-P/R/F1: 0.343/0.406/0.328	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.339/0.490/0.311 mic-P/R/F1: 0.253/0.455/0.325 TP/FP/FN: 186/550/223 std-P/R/F1: 0.351/0.425/0.323	SMAPH-S annotator

 * SVC; C = 0.1; Non-probabilistic
   * Mostly the same as above, save for the C change
   * Details:
       SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=False, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[79518  2958]
        [   24   598]]
   * Numbers:
    C2W	mac-P/R/F1: 0.394/0.698/0.424 mic-P/R/F1: 0.332/0.672/0.444 TP/FP/FN: 275/554/134 std-P/R/F1: 0.318/0.388/0.313	SMAPH-S annotator
    A2W-SAM	mac-P/R/F1: 0.276/0.458/0.276 mic-P/R/F1: 0.206/0.418/0.276 TP/FP/FN: 171/660/238 std-P/R/F1: 0.324/0.419/0.310	SMAPH-S annotator

 * SVC; C = 0.1; Probabilistic
   * models/m-no-yahoo-svc-c-0.1000-probabilistic.pkl.
   * Using 'SmaphSRemoteSvmPruner'
   * Details:
     SVC(C=0.1, cache_size=200, class_weight='balanced', coef0=0.0,
       decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
       max_iter=-1, probability=True, random_state=None, shrinking=True,
       tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[79518  2958]
        [   24   598]]
   * Numbers (threshold == 0.2):
     * Using buggy pruner!
     * C2W	mac-P/R/F1: 0.757/0.346/0.331 mic-P/R/F1: 0.548/0.264/0.356 TP/FP/FN: 108/89/301 std-P/R/F1: 0.398/0.424/0.411	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.670/0.260/0.249 mic-P/R/F1: 0.349/0.178/0.236 TP/FP/FN: 73/136/336 std-P/R/F1: 0.447/0.386/0.378	SMAPH-S annotator
   * Numbers (threshold == 0.1):
     * Using buggy pruner!
     * C2W	mac-P/R/F1: 0.615/0.428/0.379 mic-P/R/F1: 0.469/0.364/0.410 TP/FP/FN: 149/169/260 std-P/R/F1: 0.429/0.426/0.395	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.510/0.312/0.274 mic-P/R/F1: 0.301/0.252/0.274 TP/FP/FN: 103/239/306 std-P/R/F1: 0.457/0.392/0.367	SMAPH-S annotator

 * SGDClassifier
   * Back to no yahoo data :(
   * Trained on GERDAQ-{A,B} data only
   * Details:
       SGDClassifier(alpha=0.0005, average=False, class_weight='balanced',
              epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
              learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
              penalty='l1', power_t=0.5, random_state=None, shuffle=True,
              verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[77986  4490]
        [   55   567]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.297/0.723/0.363 mic-P/R/F1: 0.235/0.692/0.351 TP/FP/FN: 283/919/126 std-P/R/F1: 0.265/0.367/0.270	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.211/0.481/0.245 mic-P/R/F1: 0.151/0.445/0.226 TP/FP/FN: 182/1021/227 std-P/R/F1: 0.269/0.422/0.278	SMAPH-S annotator

 * SGDClassifier
   * Details:
       SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
              eta0=0.0, fit_intercept=True, l1_ratio=0.15,
              learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
              penalty='l1', power_t=0.5, random_state=None, shuffle=True,
              verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[69001 13475]
        [   56   566]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.239/0.740/0.294 mic-P/R/F1: 0.155/0.714/0.254 TP/FP/FN: 292/1595/117 std-P/R/F1: 0.261/0.360/0.252	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.130/0.330/0.135 mic-P/R/F1: 0.060/0.279/0.099 TP/FP/FN: 114/1777/295 std-P/R/F1: 0.250/0.389/0.226	SMAPH-S annotator

 * SGDClassifier (l2 instead of l1 penalty)
   * Details:
       SGDClassifier(alpha=0.01, average=False, class_weight='balanced', epsilon=0.1,
              eta0=0.0, fit_intercept=True, l1_ratio=0.15,
              learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,
              penalty='l2', power_t=0.5, random_state=None, shuffle=True,
              verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[78060  4416]
        [   54   568]]
   * Numbers:
     * C2W	mac-P/R/F1: 0.350/0.720/0.388 mic-P/R/F1: 0.270/0.689/0.388 TP/FP/FN: 282/764/127 std-P/R/F1: 0.296/0.370/0.284	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.223/0.416/0.223 mic-P/R/F1: 0.147/0.377/0.212 TP/FP/FN: 154/892/255 std-P/R/F1: 0.304/0.420/0.283	SMAPH-S annotator

 * Logistic Regression
   * Trained on GERDAQ-{A,B} data only
   * Using individual pruning (not probability-based)
   * Details:
       LogisticRegression(C=0.001, class_weight='balanced', dual=False,
                 fit_intercept=True, intercept_scaling=1, max_iter=100,
                 multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
                 solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[78241  4235]
        [   55   567]]
   * Numbers:
    * C2W	mac-P/R/F1: 0.372/0.701/0.405 mic-P/R/F1: 0.299/0.670/0.413 TP/FP/FN: 274/643/135 std-P/R/F1: 0.301/0.377/0.294	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.247/0.428/0.245 mic-P/R/F1: 0.173/0.389/0.240 TP/FP/FN: 159/758/250 std-P/R/F1: 0.312/0.421/0.296	SMAPH-S annotator
   * Numbers using Smaph-S probability pruning:
    * <unimportant; see more up to date numbers below>

========================================================================================================================
After fixing major Smaph-S bug
========================================================================================================================

Everything past this point always trained on GERDAQ-{A,B} data only.

 * Logistic Regression (C = 0.00005)
  * Using Smaph-S style pruning
  * <pending> unless AdaBoost is overall better (which it should be)

 * Logistic Regression (C = 0.00025)
  * Using Smaph-S style pruning
  * Trained on train-A, train-B, and devel
  * models/m-with-devel-lr-c-0.00025.pkl
  * Numbers (threshold = 0.75, with query preprocessing, splitByLP == false):
    C2W	mac-P/R/F1: 0.598/0.558/0.489 mic-P/R/F1: 0.541/0.531/0.536 TP/FP/FN: 217/184/192 std-P/R/F1: 0.417/0.409/0.394	SMAPH-S annotator
    A2W-SAM	mac-P/R/F1: 0.547/0.496/0.435 mic-P/R/F1: 0.484/0.474/0.479 TP/FP/FN: 194/207/215 std-P/R/F1: 0.431/0.413/0.397	SMAPH-S annotator

  * Numbers (threshold = 0.70, with query preprocessing, splitByLP == false):
    C2W	mac-P/R/F1: 0.588/0.581/0.504 mic-P/R/F1: 0.534/0.550/0.542 TP/FP/FN: 225/196/184 std-P/R/F1: 0.409/0.401/0.384	SMAPH-S annotator
    A2W-SAM	mac-P/R/F1: 0.523/0.500/0.435 mic-P/R/F1: 0.463/0.477/0.470 TP/FP/FN: 195/226/214 std-P/R/F1: 0.431/0.413/0.394	SMAPH-S annotator

  * Numbers (threshold = 0.65, with query preprocessing, splitByLP == false):
    * (Small) improvement on model trained without 'devel'.
    * C2W	mac-P/R/F1: 0.585/0.601/0.517 mic-P/R/F1: 0.531/0.572/0.551 TP/FP/FN: 234/207/175 std-P/R/F1: 0.402/0.395/0.379	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.513/0.512/0.441 mic-P/R/F1: 0.454/0.489/0.471 TP/FP/FN: 200/241/209 std-P/R/F1: 0.425/0.413/0.393	SMAPH-S annotator
  * Numbers (threshold = 0.60, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.564/0.611/0.511 mic-P/R/F1: 0.511/0.582/0.544 TP/FP/FN: 238/228/171 std-P/R/F1: 0.396/0.395/0.376	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.488/0.516/0.431 mic-P/R/F1: 0.433/0.494/0.462 TP/FP/FN: 202/264/207 std-P/R/F1: 0.417/0.414/0.389	SMAPH-S annotator

 * Logistic Regression (C = 0.00025)
  * Using Smaph-S style pruning
  * models/m-no-yahoo-lr-c-0.00025.pkl.
  * Details:
      LogisticRegression(C=0.00025, class_weight='balanced', dual=False,
                fit_intercept=True, intercept_scaling=1, max_iter=100,
                multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
                solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
  * Train data confusion matrix:
      [[77635  4841]
       [   56   566]]
  * Numbers (threshold = 0.80, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.614/0.539/0.478 mic-P/R/F1: 0.547/0.511/0.528 TP/FP/FN: 209/173/200 std-P/R/F1: 0.420/0.413/0.397	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.566/0.491/0.433 mic-P/R/F1: 0.479/0.467/0.473 TP/FP/FN: 191/208/218 std-P/R/F1: 0.429/0.415/0.396	SMAPH-S annotator
  * Numbers (threshold = 0.75, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.596/0.554/0.484 mic-P/R/F1: 0.538/0.523/0.530 TP/FP/FN: 214/184/195 std-P/R/F1: 0.416/0.409/0.394	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.536/0.495/0.426 mic-P/R/F1: 0.456/0.472/0.464 TP/FP/FN: 193/230/216 std-P/R/F1: 0.428/0.414/0.393	SMAPH-S annotator
  * Numbers (threshold = 0.70, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.593/0.587/0.511 mic-P/R/F1: 0.539/0.557/0.548 TP/FP/FN: 228/195/181 std-P/R/F1: 0.408/0.399/0.384	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.504/0.506/0.430 mic-P/R/F1: 0.430/0.484/0.456 TP/FP/FN: 198/262/211 std-P/R/F1: 0.421/0.411/0.387	SMAPH-S annotator

  * Numbers (threshold = 0.65, with query preprocessing, splitByLP == false):

    * C2W	mac-P/R/F1: 0.580/0.600/0.514 mic-P/R/F1: 0.525/0.570/0.546 TP/FP/FN: 233/211/176 std-P/R/F1: 0.399/0.396/0.378	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.478/0.512/0.423 mic-P/R/F1: 0.405/0.489/0.443 TP/FP/FN: 200/294/209 std-P/R/F1: 0.409/0.413/0.382	SMAPH-S annotator
    * Rechecking numbers after moving score directly to 'SmaphCandidate': A-OK!
    * By doing smarter entity trimming in the end (better A2W):
      * Confirmation of results (May 25): Confirmed.
      * C2W	mac-P/R/F1: 0.580/0.600/0.514 mic-P/R/F1: 0.525/0.570/0.546 TP/FP/FN: 233/211/176 std-P/R/F1: 0.399/0.396/0.378	SMAPH-S annotator
      * A2W-SAM	mac-P/R/F1: 0.509/0.512/0.439 mic-P/R/F1: 0.450/0.489/0.469 TP/FP/FN: 200/244/209 std-P/R/F1: 0.422/0.413/0.392	SMAPH-S annotator
      * AND also training on devel:
        * C2W	mac-P/R/F1: 0.585/0.601/0.517 mic-P/R/F1: 0.531/0.572/0.551 TP/FP/FN: 234/207/175 std-P/R/F1: 0.402/0.395/0.379	SMAPH-S annotator
        * A2W-SAM	mac-P/R/F1: 0.513/0.512/0.441 mic-P/R/F1: 0.454/0.489/0.471 TP/FP/FN: 200/241/209 std-P/R/F1: 0.425/0.413/0.393	SMAPH-S annotator

  * Numbers (threshold = 0.60, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.565/0.615/0.514 mic-P/R/F1: 0.515/0.584/0.548 TP/FP/FN: 239/225/170 std-P/R/F1: 0.394/0.394/0.375	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.447/0.516/0.409 mic-P/R/F1: 0.378/0.494/0.428 TP/FP/FN: 202/333/207 std-P/R/F1: 0.399/0.414/0.378	SMAPH-S annotator
  * Numbers (threshold = 0.55, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.521/0.616/0.493 mic-P/R/F1: 0.488/0.587/0.533 TP/FP/FN: 240/252/169 std-P/R/F1: 0.387/0.393/0.371	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.401/0.517/0.387 mic-P/R/F1: 0.354/0.496/0.413 TP/FP/FN: 203/371/206 std-P/R/F1: 0.378/0.414/0.367	SMAPH-S annotator
  * Numbers (threshold = 0.50, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.506/0.628/0.494 mic-P/R/F1: 0.472/0.594/0.526 TP/FP/FN: 243/272/166 std-P/R/F1: 0.380/0.390/0.366	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.372/0.521/0.376 mic-P/R/F1: 0.332/0.499/0.399 TP/FP/FN: 204/410/205 std-P/R/F1: 0.360/0.413/0.359	SMAPH-S annotator
  * Numbers (threshold = 0.40, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.483/0.655/0.500 mic-P/R/F1: 0.460/0.619/0.528 TP/FP/FN: 253/297/156 std-P/R/F1: 0.366/0.379/0.355	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.329/0.530/0.365 mic-P/R/F1: 0.302/0.506/0.378 TP/FP/FN: 207/479/202 std-P/R/F1: 0.333/0.413/0.349	SMAPH-S annotator
  * Numbers (threshold = 0.30, with query preprocessing, splitByLP == false):
    * C2W	mac-P/R/F1: 0.473/0.659/0.497 mic-P/R/F1: 0.452/0.621/0.523 TP/FP/FN: 254/308/155 std-P/R/F1: 0.363/0.377/0.353	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.316/0.530/0.357 mic-P/R/F1: 0.291/0.506/0.369 TP/FP/FN: 207/505/202 std-P/R/F1: 0.325/0.413/0.345	SMAPH-S annotator

   * Numbers on Y! data subsample (threshold = 0.65, w/ preprocessing):
    * C2W	mac-P/R/F1: 0.731/0.699/0.683 mic-P/R/F1: 0.688/0.632/0.659 TP/FP/FN: 665/301/387 std-P/R/F1: 0.368/0.374/0.361	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.631/0.590/0.585 mic-P/R/F1: 0.591/0.541/0.565 TP/FP/FN: 571/395/484 std-P/R/F1: 0.424/0.417/0.411	SMAPH-S annotator
   * Numbers on Y! data subsample (after no-url-splitting improvement):
    * C2W	mac-P/R/F1: 0.732/0.680/0.665 mic-P/R/F1: 0.686/0.622/0.652 TP/FP/FN: 654/300/398 std-P/R/F1: 0.368/0.383/0.370	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.650/0.590/0.585 mic-P/R/F1: 0.599/0.541/0.568 TP/FP/FN: 571/383/484 std-P/R/F1: 0.418/0.417/0.411	SMAPH-S annotator
   * Numbers on Y! data subsample (same params, but also trained on devel):
    * C2W	mac-P/R/F1: 0.730/0.688/0.677 mic-P/R/F1: 0.688/0.627/0.656 TP/FP/FN: 660/300/392 std-P/R/F1: 0.373/0.378/0.367	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.638/0.593/0.589 mic-P/R/F1: 0.598/0.544/0.570 TP/FP/FN: 574/386/481 std-P/R/F1: 0.423/0.416/0.411	SMAPH-S annotator


 * Logistic Regression (C = 0.00100)
   * Using Smaph-S style pruning
   * Unless mentioned, no query preprocessing used.
   * Details:
       LogisticRegression(C=0.001, class_weight='balanced', dual=False,
                 fit_intercept=True, intercept_scaling=1, max_iter=100,
                 multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
                 solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[78241  4235]
        [   55   567]]
   * Numbers (threshold = 0.85, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.607/0.527/0.469 mic-P/R/F1: 0.537/0.501/0.518 TP/FP/FN: 205/177/204 std-P/R/F1: 0.428/0.415/0.404	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.569/0.495/0.436 mic-P/R/F1: 0.484/0.467/0.475 TP/FP/FN: 191/204/218 std-P/R/F1: 0.430/0.415/0.399	SMAPH-S annotator
   * Numbers (threshold = 0.80, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.599/0.562/0.496 mic-P/R/F1: 0.538/0.531/0.534 TP/FP/FN: 217/186/192 std-P/R/F1: 0.419/0.407/0.396	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.538/0.503/0.439 mic-P/R/F1: 0.463/0.474/0.469 TP/FP/FN: 194/225/215 std-P/R/F1: 0.430/0.414/0.397	SMAPH-S annotator
   * Numbers (threshold = 0.75, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.589/0.581/0.508 mic-P/R/F1: 0.529/0.550/0.540 TP/FP/FN: 225/200/184 std-P/R/F1: 0.413/0.399/0.384	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.512/0.512/0.439 mic-P/R/F1: 0.439/0.484/0.460 TP/FP/FN: 198/253/211 std-P/R/F1: 0.421/0.410/0.388	SMAPH-S annotator
   * Numbers (threshold = 0.70, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.578/0.592/0.508 mic-P/R/F1: 0.523/0.562/0.542 TP/FP/FN: 230/210/179 std-P/R/F1: 0.409/0.400/0.386	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.490/0.516/0.429 mic-P/R/F1: 0.422/0.489/0.453 TP/FP/FN: 200/274/209 std-P/R/F1: 0.414/0.412/0.386	SMAPH-S annotator
   * Numbers (threshold = 0.65, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.561/0.594/0.501 mic-P/R/F1: 0.507/0.565/0.534 TP/FP/FN: 231/225/178 std-P/R/F1: 0.406/0.398/0.383	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.464/0.516/0.416 mic-P/R/F1: 0.398/0.489/0.439 TP/FP/FN: 200/303/209 std-P/R/F1: 0.407/0.412/0.381	SMAPH-S annotator
   * Numbers (threshold = 0.60, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.532/0.604/0.490 mic-P/R/F1: 0.490/0.577/0.530 TP/FP/FN: 236/246/173 std-P/R/F1: 0.397/0.396/0.378	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.427/0.521/0.399 mic-P/R/F1: 0.379/0.496/0.430 TP/FP/FN: 203/332/206 std-P/R/F1: 0.389/0.412/0.372	SMAPH-S annotator
   * Numbers (threshold = 0.55, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.514/0.617/0.493 mic-P/R/F1: 0.475/0.589/0.526 TP/FP/FN: 241/266/168 std-P/R/F1: 0.391/0.395/0.375	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.394/0.522/0.389 mic-P/R/F1: 0.356/0.499/0.415 TP/FP/FN: 204/369/205 std-P/R/F1: 0.373/0.412/0.366	SMAPH-S annotator
   * Numbers (threshold = 0.50, with query preprocessing, splitByLP == false):
     * C2W	mac-P/R/F1: 0.501/0.626/0.490 mic-P/R/F1: 0.460/0.597/0.520 TP/FP/FN: 244/286/165 std-P/R/F1: 0.382/0.390/0.368	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.373/0.526/0.380 mic-P/R/F1: 0.338/0.501/0.404 TP/FP/FN: 205/402/204 std-P/R/F1: 0.359/0.412/0.359	SMAPH-S annotator
   * Numbers (threshold = 0.45):
     * C2W	mac-P/R/F1: 0.496/0.624/0.489 mic-P/R/F1: 0.448/0.592/0.510 TP/FP/FN: 242/298/167 std-P/R/F1: 0.376/0.393/0.364	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.368/0.516/0.376 mic-P/R/F1: 0.319/0.489/0.386 TP/FP/FN: 200/426/209 std-P/R/F1: 0.355/0.409/0.355	SMAPH-S annotator
   * Numbers (threshold = 0.40):
     * C2W	mac-P/R/F1: 0.487/0.634/0.489 mic-P/R/F1: 0.443/0.604/0.511 TP/FP/FN: 247/310/162 std-P/R/F1: 0.368/0.389/0.358	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.355/0.518/0.371 mic-P/R/F1: 0.310/0.491/0.380 TP/FP/FN: 201/448/208 std-P/R/F1: 0.345/0.409/0.349	SMAPH-S annotator
     * After merging Berni's changes but not using them: CONFIRMED SAME NUMBERS
     * After also using Berni's util before submitting to Bing (splitByLP == false):
       * C2W	mac-P/R/F1: 0.482/0.649/0.498 mic-P/R/F1: 0.447/0.619/0.519 TP/FP/FN: 253/313/156 std-P/R/F1: 0.372/0.381/0.361	SMAPH-S annotator
       * A2W-SAM	mac-P/R/F1: 0.344/0.536/0.377 mic-P/R/F1: 0.319/0.509/0.392 TP/FP/FN: 208/445/201 std-P/R/F1: 0.341/0.411/0.354	SMAPH-S annotator
     * After also using Berni's util + splitByLP == true (slightly worse):
       * C2W	mac-P/R/F1: 0.468/0.645/0.491 mic-P/R/F1: 0.439/0.616/0.513 TP/FP/FN: 252/322/157 std-P/R/F1: 0.366/0.382/0.358	SMAPH-S annotator
       * A2W-SAM	mac-P/R/F1: 0.338/0.533/0.374 mic-P/R/F1: 0.312/0.509/0.387 TP/FP/FN: 208/459/201 std-P/R/F1: 0.337/0.410/0.351	SMAPH-S annotator
   * Numbers (threshold = 0.35):
     * C2W	mac-P/R/F1: 0.468/0.636/0.485 mic-P/R/F1: 0.437/0.606/0.508 TP/FP/FN: 248/320/161 std-P/R/F1: 0.363/0.387/0.355	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.333/0.520/0.363 mic-P/R/F1: 0.297/0.494/0.371 TP/FP/FN: 202/477/207 std-P/R/F1: 0.334/0.407/0.344	SMAPH-S annotator
   * Numbers (threshold = 0.30):
     * C2W	mac-P/R/F1: 0.461/0.636/0.479 mic-P/R/F1: 0.430/0.606/0.503 TP/FP/FN: 248/329/161 std-P/R/F1: 0.363/0.387/0.354	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.326/0.520/0.356 mic-P/R/F1: 0.289/0.494/0.365 TP/FP/FN: 202/496/207 std-P/R/F1: 0.332/0.407/0.342	SMAPH-S annotator
   * Numbers (threshold = 0.25):
      C2W	mac-P/R/F1: 0.460/0.640/0.479 mic-P/R/F1: 0.426/0.609/0.502 TP/FP/FN: 249/335/160 std-P/R/F1: 0.360/0.386/0.352	SMAPH-S annotator
      A2W-SAM	mac-P/R/F1: 0.323/0.520/0.355 mic-P/R/F1: 0.285/0.494/0.361 TP/FP/FN: 202/508/207 std-P/R/F1: 0.330/0.407/0.341	SMAPH-S annotator
   * Numbers (threshold = 0.20):
     * C2W	mac-P/R/F1: 0.457/0.640/0.478 mic-P/R/F1: 0.423/0.609/0.499 TP/FP/FN: 249/339/160 std-P/R/F1: 0.359/0.386/0.352	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.320/0.520/0.353 mic-P/R/F1: 0.281/0.494/0.358 TP/FP/FN: 202/516/207 std-P/R/F1: 0.327/0.407/0.340	SMAPH-S annotator
   * Numbers (threshold = 0.15):
     * C2W	mac-P/R/F1: 0.448/0.640/0.469 mic-P/R/F1: 0.421/0.609/0.498 TP/FP/FN: 249/342/160 std-P/R/F1: 0.357/0.386/0.351	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.311/0.520/0.345 mic-P/R/F1: 0.280/0.494/0.357 TP/FP/FN: 202/520/207 std-P/R/F1: 0.322/0.407/0.336	SMAPH-S annotator
   * Numbers (threshold = 0.10):
     * C2W	mac-P/R/F1: 0.446/0.640/0.469 mic-P/R/F1: 0.421/0.609/0.498 TP/FP/FN: 249/343/160 std-P/R/F1: 0.355/0.386/0.350	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.311/0.520/0.345 mic-P/R/F1: 0.279/0.494/0.357 TP/FP/FN: 202/521/207 std-P/R/F1: 0.322/0.407/0.336	SMAPH-S annotator
   * Numbers (threshold = 0.05):
     * C2W	mac-P/R/F1: 0.446/0.640/0.469 mic-P/R/F1: 0.421/0.609/0.498 TP/FP/FN: 249/343/160 std-P/R/F1: 0.355/0.386/0.350	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.311/0.520/0.345 mic-P/R/F1: 0.279/0.494/0.357 TP/FP/FN: 202/521/207 std-P/R/F1: 0.322/0.407/0.336	SMAPH-S annotator
     * Odd. Gotta rerun.
     * Confirmed. Strange that the change is so small.

 * Logistic Regression (C == 0.01)
   * models/m-no-yahoo-lr-c-0.01.pkl
   * Details:
       LogisticRegression(C=0.01, class_weight='balanced', dual=False,
                 fit_intercept=True, intercept_scaling=1, max_iter=100,
                 multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
                 solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[78203  4273]
        [   55   567]]
   * Numbers (threshold = 0.80, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.568/0.577/0.495 mic-P/R/F1: 0.515/0.545/0.530 TP/FP/FN: 223/210/186 std-P/R/F1: 0.420/0.407/0.393	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.502/0.514/0.434 mic-P/R/F1: 0.437/0.484/0.459 TP/FP/FN: 198/255/211 std-P/R/F1: 0.426/0.416/0.394	SMAPH-S annotator
   * Numbers (threshold = 0.75, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.550/0.584/0.492 mic-P/R/F1: 0.499/0.553/0.524 TP/FP/FN: 226/227/183 std-P/R/F1: 0.414/0.404/0.390	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.472/0.514/0.421 mic-P/R/F1: 0.412/0.484/0.445 TP/FP/FN: 198/283/211 std-P/R/F1: 0.416/0.416/0.390	SMAPH-S annotator
   * Numbers (threshold = 0.70, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.541/0.591/0.489 mic-P/R/F1: 0.488/0.562/0.523 TP/FP/FN: 230/241/179 std-P/R/F1: 0.411/0.405/0.390	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.456/0.520/0.414 mic-P/R/F1: 0.397/0.491/0.439 TP/FP/FN: 201/305/208 std-P/R/F1: 0.408/0.417/0.387	SMAPH-S annotator
   * Numbers (threshold = 0.65, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.512/0.602/0.483 mic-P/R/F1: 0.474/0.575/0.519 TP/FP/FN: 235/261/174 std-P/R/F1: 0.404/0.403/0.387	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.417/0.524/0.399 mic-P/R/F1: 0.381/0.499/0.432 TP/FP/FN: 204/332/205 std-P/R/F1: 0.392/0.418/0.381	SMAPH-S annotator
   * Numbers (threshold = 0.60, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.503/0.604/0.479 mic-P/R/F1: 0.460/0.579/0.513 TP/FP/FN: 237/278/172 std-P/R/F1: 0.400/0.403/0.384	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.405/0.524/0.393 mic-P/R/F1: 0.364/0.499/0.421 TP/FP/FN: 204/357/205 std-P/R/F1: 0.386/0.418/0.377	SMAPH-S annotator
   * Numbers (threshold = 0.55, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.486/0.612/0.478 mic-P/R/F1: 0.446/0.584/0.506 TP/FP/FN: 239/297/170 std-P/R/F1: 0.391/0.401/0.378	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.381/0.524/0.384 mic-P/R/F1: 0.348/0.499/0.410 TP/FP/FN: 204/382/205 std-P/R/F1: 0.372/0.418/0.371	SMAPH-S annotator
   * Numbers (threshold = 0.50, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.480/0.616/0.477 mic-P/R/F1: 0.440/0.592/0.505 TP/FP/FN: 242/308/167 std-P/R/F1: 0.386/0.399/0.375	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.366/0.525/0.379 mic-P/R/F1: 0.337/0.501/0.403 TP/FP/FN: 205/403/204 std-P/R/F1: 0.363/0.419/0.367	SMAPH-S annotator
   * Numbers (threshold = 0.45, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.473/0.624/0.476 mic-P/R/F1: 0.434/0.601/0.504 TP/FP/FN: 246/321/163 std-P/R/F1: 0.380/0.397/0.371	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.357/0.526/0.374 mic-P/R/F1: 0.325/0.504/0.395 TP/FP/FN: 206/428/203 std-P/R/F1: 0.357/0.419/0.364	SMAPH-S annotator
   * Numbers (threshold = 0.40, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.464/0.632/0.478 mic-P/R/F1: 0.429/0.606/0.503 TP/FP/FN: 248/330/161 std-P/R/F1: 0.374/0.395/0.366	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.348/0.535/0.376 mic-P/R/F1: 0.321/0.509/0.394 TP/FP/FN: 208/440/201 std-P/R/F1: 0.350/0.419/0.361	SMAPH-S annotator

 * Logistic Regression (C == 0.1)
   * models/m-no-yahoo-lr-c-0.1.pkl
   * Details:
       LogisticRegression(C=0.1, class_weight='balanced', dual=False,
                 fit_intercept=True, intercept_scaling=1, max_iter=100,
                 multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
                 solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
   * Train data confusion matrix:
       [[78148  4328]
        [   52   570]]
   * Numbers (threshold = 0.55, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.481/0.609/0.474 mic-P/R/F1: 0.441/0.582/0.502 TP/FP/FN: 238/302/171 std-P/R/F1: 0.392/0.404/0.380	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.377/0.524/0.382 mic-P/R/F1: 0.346/0.499/0.408 TP/FP/FN: 204/386/205 std-P/R/F1: 0.371/0.421/0.371	SMAPH-S annotator
   * Numbers (threshold = 0.50, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.477/0.616/0.475 mic-P/R/F1: 0.434/0.589/0.500 TP/FP/FN: 241/314/168 std-P/R/F1: 0.388/0.401/0.376	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.363/0.524/0.377 mic-P/R/F1: 0.333/0.499/0.399 TP/FP/FN: 204/409/205 std-P/R/F1: 0.363/0.421/0.368	SMAPH-S annotator
   * Numbers (threshold = 0.45, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.473/0.617/0.474 mic-P/R/F1: 0.429/0.592/0.497 TP/FP/FN: 242/322/167 std-P/R/F1: 0.385/0.401/0.375	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.357/0.524/0.373 mic-P/R/F1: 0.323/0.499/0.392 TP/FP/FN: 204/428/205 std-P/R/F1: 0.361/0.421/0.366	SMAPH-S annotator
   * Numbers (threshold = 0.40, with query preprocessing, splitByLP == false)):
     * C2W	mac-P/R/F1: 0.464/0.626/0.473 mic-P/R/F1: 0.422/0.599/0.495 TP/FP/FN: 245/336/164 std-P/R/F1: 0.378/0.398/0.369	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.348/0.534/0.372 mic-P/R/F1: 0.316/0.506/0.389 TP/FP/FN: 207/448/202 std-P/R/F1: 0.353/0.421/0.361	SMAPH-S annotator


 * SVC (C == 1.0):
   * Data may have been affected by existing smaph-S selection bug.
   * models/m-no-yahoo-svc-c-1.0000-probabilistic.pkl
   * Details:
       SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,
         decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
         max_iter=-1, probability=True, random_state=None, shrinking=True,
         tol=0.001, verbose=False)
   * Train data confusion matrix:
       [[80644  1832]
        [    4   618]]
   * Numbers (threshold = 0.20):
     * C2W	mac-P/R/F1: 0.629/0.426/0.389 mic-P/R/F1: 0.505/0.367/0.425 TP/FP/FN: 150/147/259 std-P/R/F1: 0.421/0.419/0.395	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.557/0.353/0.321 mic-P/R/F1: 0.386/0.293/0.333 TP/FP/FN: 120/191/289 std-P/R/F1: 0.446/0.405/0.384	SMAPH-S annotator
   * Numbers (threshold = 0.15):
     * C2W	mac-P/R/F1: 0.573/0.459/0.398 mic-P/R/F1: 0.483/0.411/0.444 TP/FP/FN: 168/180/241 std-P/R/F1: 0.419/0.414/0.387	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.496/0.378/0.324 mic-P/R/F1: 0.366/0.328/0.346 TP/FP/FN: 134/232/275 std-P/R/F1: 0.434/0.405/0.377	SMAPH-S annotator
   * Numbers (threshold = 0.125):
     * C2W	mac-P/R/F1: 0.554/0.470/0.405 mic-P/R/F1: 0.475/0.425/0.449 TP/FP/FN: 174/192/235 std-P/R/F1: 0.414/0.413/0.384	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.476/0.385/0.328 mic-P/R/F1: 0.357/0.337/0.347 TP/FP/FN: 138/249/271 std-P/R/F1: 0.428/0.403/0.372	SMAPH-S annotator
   * Numbers (threshold = 0.10):
     * C2W	mac-P/R/F1: 0.546/0.499/0.424 mic-P/R/F1: 0.477/0.457/0.467 TP/FP/FN: 187/205/222 std-P/R/F1: 0.411/0.417/0.388	SMAPH-S annotator
     * A2W-SAM	mac-P/R/F1: 0.462/0.407/0.341 mic-P/R/F1: 0.359/0.364/0.362 TP/FP/FN: 149/266/260 std-P/R/F1: 0.423/0.410/0.379	SMAPH-S annotator
