# First results of the SMAPH-1 implementation (which also uses some SMAPH-S elements)
 * SVM: nonlinear SVC with RBF kernel and C = 1.
 * WAT: used defaults everywhere
 * May 11 2016
 * Numbers:
    * C2W	mac-P/R/F1: 0.379/0.637/0.396 mic-P/R/F1: 0.305/0.584/0.401 TP/FP/FN: 239/544/170 std-P/R/F1: 0.315/0.387/0.300	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.124/0.177/0.087 mic-P/R/F1: 0.032/0.061/0.042 TP/FP/FN: 25/759/384 std-P/R/F1: 0.299/0.364/0.238	SMAPH-S annotator

# First results of the SMAPH-1 implementation (which also uses some SMAPH-S elements)
 * Same config as above.
 * NO pruning performed.
 * May 14 2016
 * Time taken: ~6 minutes (everything cached)
 * Numbers:
    * C2W	mac-P/R/F1: 0.131/0.765/0.194 mic-P/R/F1: 0.099/0.731/0.175 TP/FP/FN: 299/2709/110 std-P/R/F1: 0.160/0.343/0.170	SMAPH-S annotator
    * A2W-SAM	mac-P/R/F1: 0.039/0.218/0.043 mic-P/R/F1: 0.014/0.103/0.025 TP/FP/FN: 42/2972/367 std-P/R/F1: 0.149/0.382/0.136	SMAPH-S annotator
 * Notes:
    * As expected, recall is quite higher, but not by much. Perhaps we could tune WAT for even
    more recall, since 0.765 is not all that much.
    * C2W precision dropped, but not as much as I (andrei) personally expected.
    * Nevertheless, we should experiment with stricter pruning in the SVM classifier.
    * But first, let's train on BOTH datasets (train A and train B).

# Results using basically the same implementation as above, but trained on both GERDAQ-A and B
 * With pruning.
 * May 14 2016
 * Time taken: ~6 minutes (everything cached as before, but populating higher level cache)
 * Numbers:
   * C2W	mac-P/R/F1: 0.403/0.645/0.415 mic-P/R/F1: 0.320/0.592/0.415 TP/FP/FN: 242/514/167 std-P/R/F1: 0.322/0.388/0.308	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.151/0.190/0.111 mic-P/R/F1: 0.042/0.078/0.055 TP/FP/FN: 32/725/377 std-P/R/F1: 0.322/0.368/0.269	SMAPH-S annotator

# Same as before, but C = 0.01
 * Time taken: ~12 minutes
 * C = 0.01 in the kernelized SVM; precision way lower than before with C = 1;
 * Numbers:
   * C2W	mac-P/R/F1: 0.339/0.653/0.395 mic-P/R/F1: 0.274/0.606/0.378 TP/FP/FN: 248/656/161 std-P/R/F1: 0.291/0.382/0.299	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.090/0.176/0.087 mic-P/R/F1: 0.029/0.064/0.040 TP/FP/FN: 26/880/383 std-P/R/F1: 0.249/0.360/0.239	SMAPH-S annotator

* C = 2.0
 * Time taken: ~14 minutes (other stuff going on)
 * C = 2.0
 * Numbers:
   * C2W	mac-P/R/F1: 0.404/0.639/0.414 mic-P/R/F1: 0.322/0.587/0.416 TP/FP/FN: 240/505/169 std-P/R/F1: 0.326/0.391/0.311	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.140/0.172/0.097 mic-P/R/F1: 0.032/0.059/0.042 TP/FP/FN: 24/722/385 std-P/R/F1: 0.320/0.359/0.262	SMAPH-S annotator

* C = 3.0
 * Numbers:
   * C2W	mac-P/R/F1: 0.402/0.632/0.413 mic-P/R/F1: 0.323/0.584/0.416 TP/FP/FN: 239/502/170 std-P/R/F1: 0.330/0.397/0.317	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.134/0.166/0.094 mic-P/R/F1: 0.030/0.054/0.038 TP/FP/FN: 22/720/387 std-P/R/F1: 0.315/0.355/0.261	SMAPH-S annotator

* C = 0.5
 * Numbers:
   * C2W	mac-P/R/F1: 0.397/0.647/0.416 mic-P/R/F1: 0.318/0.594/0.414 TP/FP/FN: 243/521/166 std-P/R/F1: 0.318/0.386/0.306	SMAPH-S annotator
   * A2W-SAM	mac-P/R/F1: 0.136/0.174/0.098 mic-P/R/F1: 0.033/0.061/0.043 TP/FP/FN: 25/740/384 std-P/R/F1: 0.312/0.360/0.260	SMAPH-S annotator